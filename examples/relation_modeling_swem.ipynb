{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/kogito/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-03-22 01:11:18.020344: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-22 01:11:18.020382: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import wandb\n",
    "import spacy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_emb_matrix(embedding_dim=100):\n",
    "    glove = pd.read_csv(f'data/glove/glove.6B.{embedding_dim}d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    vocab = {'<pad>': 0, '<unk>': 1}\n",
    "    embeddings = np.zeros((len(glove) + 2, embedding_dim))\n",
    "    embeddings[0] = np.zeros(embedding_dim)\n",
    "    embeddings[1] = np.zeros(embedding_dim)\n",
    "\n",
    "    for index, (key, val) in tqdm(enumerate(glove.T.items())):\n",
    "        vocab[key] = index + 2\n",
    "        embeddings[index+2] = val.to_numpy()\n",
    "\n",
    "    return vocab, embeddings\n",
    "    \n",
    "\n",
    "class HeadDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.labels = df['label'].to_numpy()\n",
    "        self.texts = pad_sequence([torch.tensor([vocab.get(token.text, 1) for token in nlp(text)], dtype=torch.int) for text in df['text']], batch_first=True)\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "\n",
    "class MaxPool(nn.Module):\n",
    "    def forward(self, X):\n",
    "        values, _ = torch.max(X, dim=1)\n",
    "        return values\n",
    "\n",
    "\n",
    "class AvgPool(nn.Module):\n",
    "    def forward(self, X):\n",
    "        return torch.mean(X, dim=1)\n",
    "\n",
    "\n",
    "class SWEMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=3, pooling=\"max\", embedding_matrix=None, freeze_emb=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=embedding_matrix.shape[0],\n",
    "                                      embedding_dim=embedding_matrix.shape[1]).from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=freeze_emb)\n",
    "        self.pool = MaxPool() if pooling == \"max\" else AvgPool()\n",
    "        self.linear = nn.Linear(embedding_matrix.shape[1], num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        outputs = self.embedding(X)\n",
    "        outputs = self.pool(outputs)\n",
    "        outputs = self.linear(outputs)\n",
    "        outputs = self.softmax(outputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def save_pretrained(self, path):\n",
    "        torch.save(self, path)\n",
    "\n",
    "\n",
    "def train(model, train_dataset, val_dataset, learning_rate=1e-3, epochs=10, batch_size=8):\n",
    "    # wandb.init(project=\"kogito-relation-matcher\", config={\"learning_rate\": learning_rate, \"epochs\": epochs, \"batch_size\": batch_size})\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        print(\"Using CUDA\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            model.zero_grad()\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            X = train_input.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            \n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "            \n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_input, val_label in val_dataloader:\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "                X = val_input.to(device)\n",
    "\n",
    "                output = model(X)\n",
    "\n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        train_loss = total_loss_train / len(train_dataset)\n",
    "        train_acc = total_acc_train / len(train_dataset)\n",
    "        val_loss = total_loss_val / len(val_dataset)\n",
    "        val_acc = total_acc_val / len(val_dataset)\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {train_loss: .3f} \\\n",
    "            | Train Accuracy: {train_acc: .3f} \\\n",
    "            | Val Loss: {val_loss: .3f} \\\n",
    "            | Val Accuracy: {val_acc: .3f}')\n",
    "        \n",
    "        # wandb.log({\"train_loss\": train_loss, \"train_accuracy\": train_acc, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
    "        # model.save_pretrained(f\"./models/checkpoint_{epoch_num}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:11, 35416.62it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab, emb_matrix = create_emb_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relation_modeling_utils import load_data\n",
    "\n",
    "train_df = load_data(\"data/atomic2020_data-feb2021/train.tsv\")\n",
    "dev_df = load_data(\"data/atomic2020_data-feb2021/dev.tsv\")\n",
    "train_data = HeadDataset(train_df, vocab=vocab)\n",
    "val_data = HeadDataset(dev_df, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 61.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.007             | Train Accuracy:  0.610             | Val Loss:  0.005             | Val Accuracy:  0.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 61.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.005             | Train Accuracy:  0.794             | Val Loss:  0.004             | Val Accuracy:  0.834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 60.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.004             | Train Accuracy:  0.823             | Val Loss:  0.004             | Val Accuracy:  0.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 60.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.004             | Train Accuracy:  0.838             | Val Loss:  0.003             | Val Accuracy:  0.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 60.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.003             | Train Accuracy:  0.846             | Val Loss:  0.003             | Val Accuracy:  0.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 60.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.003             | Train Accuracy:  0.850             | Val Loss:  0.003             | Val Accuracy:  0.857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 61.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.003             | Train Accuracy:  0.853             | Val Loss:  0.003             | Val Accuracy:  0.861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 61.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.003             | Train Accuracy:  0.855             | Val Loss:  0.003             | Val Accuracy:  0.861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 61.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.003             | Train Accuracy:  0.856             | Val Loss:  0.003             | Val Accuracy:  0.861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 61.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.003             | Train Accuracy:  0.858             | Val Loss:  0.003             | Val Accuracy:  0.861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 61.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.003             | Train Accuracy:  0.859             | Val Loss:  0.003             | Val Accuracy:  0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 61.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss:  0.003             | Train Accuracy:  0.860             | Val Loss:  0.003             | Val Accuracy:  0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 61.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss:  0.003             | Train Accuracy:  0.861             | Val Loss:  0.003             | Val Accuracy:  0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 61.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Train Loss:  0.003             | Train Accuracy:  0.862             | Val Loss:  0.003             | Val Accuracy:  0.859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 60.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | Train Loss:  0.003             | Train Accuracy:  0.864             | Val Loss:  0.003             | Val Accuracy:  0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 62.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 16 | Train Loss:  0.003             | Train Accuracy:  0.866             | Val Loss:  0.003             | Val Accuracy:  0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 62.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 17 | Train Loss:  0.003             | Train Accuracy:  0.867             | Val Loss:  0.003             | Val Accuracy:  0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 62.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 18 | Train Loss:  0.003             | Train Accuracy:  0.869             | Val Loss:  0.003             | Val Accuracy:  0.862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 62.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 19 | Train Loss:  0.003             | Train Accuracy:  0.870             | Val Loss:  0.003             | Val Accuracy:  0.857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:04<00:00, 62.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 20 | Train Loss:  0.003             | Train Accuracy:  0.871             | Val Loss:  0.003             | Val Accuracy:  0.857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = SWEMClassifier(embedding_matrix=emb_matrix, pooling=\"max\", freeze_emb=False)\n",
    "train(model=model, train_dataset=train_data, val_dataset=val_data, epochs=20, batch_size=128, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
    "\n",
    "def evaluate(val_dataset):\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_input, val_label = next(iter(val_dataloader))\n",
    "        X = val_input.to(device)\n",
    "        # val_label = val_label.to(device)\n",
    "        outputs = model(X)\n",
    "        preds = outputs.argmax(dim=1).detach().cpu()\n",
    "        accuracy = Accuracy()(preds, val_label)\n",
    "        precision = Precision(num_classes=3, average=\"weighted\")(preds, val_label)\n",
    "        recall = Recall(num_classes=3, average=\"weighted\")(preds, val_label)\n",
    "        f1score = F1Score(num_classes=3, average=\"weighted\")(preds, val_label)\n",
    "    \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': tensor(0.8569),\n",
       " 'precision': tensor(0.7950),\n",
       " 'recall': tensor(0.8569),\n",
       " 'f1_score': tensor(0.8129)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(val_data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c3b128559c7e8fd624042ca8b6c93b33cd59aca7b58d05c9d4cd21ec1a84d35"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('kogito')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
