{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import wandb\n",
    "import spacy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_emb_matrix(embedding_dim=100):\n",
    "    glove = pd.read_csv(f'data/glove/glove.6B.{embedding_dim}d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    vocab = {'<pad>': 0, '<unk>': 1}\n",
    "    embeddings = np.zeros((len(glove) + 2, embedding_dim))\n",
    "    embeddings[0] = np.zeros(embedding_dim)\n",
    "    embeddings[1] = np.zeros(embedding_dim)\n",
    "\n",
    "    for index, (key, val) in tqdm(enumerate(glove.T.items())):\n",
    "        vocab[key] = index + 2\n",
    "        embeddings[index+2] = val.to_numpy()\n",
    "\n",
    "    return vocab, embeddings\n",
    "    \n",
    "\n",
    "class HeadDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.labels = df['label'].to_numpy()\n",
    "        self.texts = pad_sequence([torch.tensor([vocab.get(token.text, 1) for token in nlp(text)], dtype=torch.int) for text in df['text']], batch_first=True)\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "\n",
    "class MaxPool(nn.Module):\n",
    "    def forward(self, X):\n",
    "        values, _ = torch.max(X, dim=1)\n",
    "        return values\n",
    "\n",
    "\n",
    "class AvgPool(nn.Module):\n",
    "    def forward(self, X):\n",
    "        return torch.mean(X, dim=1)\n",
    "\n",
    "\n",
    "class SWEMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=128, num_classes=3, pooling=\"max\", embedding_matrix=None, freeze_emb=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=embedding_matrix.shape[0],\n",
    "                                      embedding_dim=embedding_matrix.shape[1]).from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=freeze_emb)\n",
    "        self.pool = MaxPool() if pooling == \"max\" else AvgPool()\n",
    "        self.linear1 = nn.Linear(embedding_matrix.shape[1], hidden_dim)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        outputs = self.embedding(X)\n",
    "        outputs = self.pool(outputs)\n",
    "        outputs = self.linear1(outputs)\n",
    "        outputs = self.activation1(outputs)\n",
    "        outputs = self.linear2(outputs)\n",
    "        outputs = self.softmax(outputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def save_pretrained(self, path):\n",
    "        torch.save(self, path)\n",
    "\n",
    "\n",
    "def train(model, train_dataset, val_dataset, learning_rate=1e-3, epochs=10, batch_size=8):\n",
    "    # wandb.init(project=\"kogito-relation-matcher\", config={\"learning_rate\": learning_rate, \"epochs\": epochs, \"batch_size\": batch_size})\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        print(\"Using CUDA\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            model.zero_grad()\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            X = train_input.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            \n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "            \n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_input, val_label in val_dataloader:\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "                X = val_input.to(device)\n",
    "\n",
    "                output = model(X)\n",
    "\n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        train_loss = total_loss_train / len(train_data)\n",
    "        train_acc = total_acc_train / len(train_data)\n",
    "        val_loss = total_loss_val / len(val_data)\n",
    "        val_acc = total_acc_val / len(val_data)\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {train_loss: .3f} \\\n",
    "            | Train Accuracy: {train_acc: .3f} \\\n",
    "            | Val Loss: {val_loss: .3f} \\\n",
    "            | Val Accuracy: {val_acc: .3f}')\n",
    "        \n",
    "        # wandb.log({\"train_loss\": train_loss, \"train_accuracy\": train_acc, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
    "        # model.save_pretrained(f\"./models/checkpoint_{epoch_num}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:11, 35315.74it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab, emb_matrix = create_emb_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relation_modeling_utils import load_data\n",
    "\n",
    "train_df = load_data(\"data/atomic2020_data-feb2021/train.tsv\")\n",
    "dev_df = load_data(\"data/atomic2020_data-feb2021/dev.tsv\")\n",
    "train_data = HeadDataset(train_df, vocab=vocab)\n",
    "val_data = HeadDataset(dev_df, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 422/422 [00:06<00:00, 61.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.005             | Train Accuracy:  0.618             | Val Loss:  0.005             | Val Accuracy:  0.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 77/422 [00:01<00:05, 61.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/kogito/examples/relation_modeling_swem.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biccluster072.iccluster.epfl.ch/root/kogito/examples/relation_modeling_swem.ipynb#ch0000004vscode-remote?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m SWEMClassifier(embedding_matrix\u001b[39m=\u001b[39memb_matrix, pooling\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m\"\u001b[39m, freeze_emb\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Biccluster072.iccluster.epfl.ch/root/kogito/examples/relation_modeling_swem.ipynb#ch0000004vscode-remote?line=1'>2</a>\u001b[0m train(model\u001b[39m=\u001b[39;49mmodel, train_dataset\u001b[39m=\u001b[39;49mtrain_data, val_dataset\u001b[39m=\u001b[39;49mval_data, epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n",
      "\u001b[1;32m/root/kogito/examples/relation_modeling_swem.ipynb Cell 1'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, val_dataset, learning_rate, epochs, batch_size)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Biccluster072.iccluster.epfl.ch/root/kogito/examples/relation_modeling_swem.ipynb#ch0000002vscode-remote?line=127'>128</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_input, train_label \u001b[39min\u001b[39;00m tqdm(train_dataloader):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Biccluster072.iccluster.epfl.ch/root/kogito/examples/relation_modeling_swem.ipynb#ch0000002vscode-remote?line=128'>129</a>\u001b[0m     model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Biccluster072.iccluster.epfl.ch/root/kogito/examples/relation_modeling_swem.ipynb#ch0000002vscode-remote?line=130'>131</a>\u001b[0m     train_label \u001b[39m=\u001b[39m train_label\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Biccluster072.iccluster.epfl.ch/root/kogito/examples/relation_modeling_swem.ipynb#ch0000002vscode-remote?line=131'>132</a>\u001b[0m     X \u001b[39m=\u001b[39m train_input\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Biccluster072.iccluster.epfl.ch/root/kogito/examples/relation_modeling_swem.ipynb#ch0000002vscode-remote?line=133'>134</a>\u001b[0m     output \u001b[39m=\u001b[39m model(X)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SWEMClassifier(embedding_matrix=emb_matrix, pooling=\"avg\", freeze_emb=False)\n",
    "train(model=model, train_dataset=train_data, val_dataset=val_data, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.label == 1].head(50)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c3b128559c7e8fd624042ca8b6c93b33cd59aca7b58d05c9d4cd21ec1a84d35"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('kogito')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
