{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def create_emb_matrix(embedding_dim=100):\n",
    "    glove = pd.read_csv(f'data/glove/glove.6B.{embedding_dim}d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    vocab = {'<pad>': 0, '<unk>': 1}\n",
    "    embeddings = np.zeros((len(glove) + 2, embedding_dim))\n",
    "    embeddings[0] = np.zeros(embedding_dim)\n",
    "    embeddings[1] = np.zeros(embedding_dim)\n",
    "\n",
    "    for index, (key, val) in tqdm(enumerate(glove.T.items())):\n",
    "        vocab[key] = index + 2\n",
    "        embeddings[index+2] = val.to_numpy()\n",
    "\n",
    "    return vocab, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, emb_matrix = create_emb_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kogito.core.relation import PHYSICAL_RELATIONS, SOCIAL_RELATIONS, EVENT_RELATIONS\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torchtext.vocab import GloVe\n",
    "import wandb\n",
    "import spacy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def load_data(datapath):\n",
    "    data = []\n",
    "    head_label_set = set()\n",
    "\n",
    "    with open(datapath) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                head, relation, _ = line.split('\\t')\n",
    "\n",
    "                label = 0 \n",
    "\n",
    "                if relation in EVENT_RELATIONS:\n",
    "                    label = 1\n",
    "                elif relation in SOCIAL_RELATIONS:\n",
    "                    label = 2\n",
    "\n",
    "                if (head, label) not in head_label_set:\n",
    "                    data.append((head, label))\n",
    "                    head_label_set.add((head, label))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return pd.DataFrame(data, columns=['text', 'label'])\n",
    "    \n",
    "\n",
    "class HeadDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.labels = df['label'].to_numpy()\n",
    "        self.texts = pad_sequence([torch.tensor([vocab.get(token.text, 1) for token in nlp(text)], dtype=torch.int) for text in df['text']], batch_first=True)\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "\n",
    "class MaxPool(nn.Module):\n",
    "    def forward(self, X):\n",
    "        values, _ = torch.max(X, dim=1)\n",
    "        return values\n",
    "\n",
    "\n",
    "class AvgPool(nn.Module):\n",
    "    def forward(self, X):\n",
    "        return torch.mean(X, dim=1)\n",
    "\n",
    "\n",
    "class SWEMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=128, num_classes=3, pooling=\"max\", embedding_matrix=None, freeze_emb=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=embedding_matrix.shape[0],\n",
    "                                      embedding_dim=embedding_matrix.shape[1]).from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=freeze_emb)\n",
    "        self.pool = MaxPool() if pooling == \"max\" else AvgPool()\n",
    "        self.linear1 = nn.Linear(embedding_matrix.shape[1], hidden_dim)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        outputs = self.embedding(X)\n",
    "        outputs = self.pool(outputs)\n",
    "        outputs = self.linear1(outputs)\n",
    "        outputs = self.activation1(outputs)\n",
    "        outputs = self.linear2(outputs)\n",
    "        outputs = self.softmax(outputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def save_pretrained(self, path):\n",
    "        torch.save(self, path)\n",
    "\n",
    "\n",
    "def train(model, train_dataset, val_dataset, learning_rate=1e-3, epochs=10, batch_size=8):\n",
    "    # wandb.init(project=\"kogito-relation-matcher\", config={\"learning_rate\": learning_rate, \"epochs\": epochs, \"batch_size\": batch_size})\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        print(\"Using CUDA\")\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            model.zero_grad()\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            X = train_input.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            \n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "            \n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_input, val_label in val_dataloader:\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "                X = val_input.to(device)\n",
    "\n",
    "                output = model(X)\n",
    "\n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        train_loss = total_loss_train / len(train_data)\n",
    "        train_acc = total_acc_train / len(train_data)\n",
    "        val_loss = total_loss_val / len(val_data)\n",
    "        val_acc = total_acc_val / len(val_data)\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {train_loss: .3f} \\\n",
    "            | Train Accuracy: {train_acc: .3f} \\\n",
    "            | Val Loss: {val_loss: .3f} \\\n",
    "            | Val Accuracy: {val_acc: .3f}')\n",
    "        \n",
    "        # wandb.log({\"train_loss\": train_loss, \"train_accuracy\": train_acc, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
    "        # model.save_pretrained(f\"./models/checkpoint_{epoch_num}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 843/843 [00:01<00:00, 580.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.014             | Train Accuracy:  0.563             | Val Loss:  0.014             | Val Accuracy:  0.502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 843/843 [00:01<00:00, 574.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.013             | Train Accuracy:  0.594             | Val Loss:  0.014             | Val Accuracy:  0.502\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data(\"data/atomic2020_data-feb2021/train.tsv\")\n",
    "dev_df = load_data(\"data/atomic2020_data-feb2021/dev.tsv\")\n",
    "train_data = HeadDataset(train_df, vocab=vocab)\n",
    "val_data = HeadDataset(dev_df, vocab=vocab)\n",
    "# model.save_pretrained(\"./models/final_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 843/843 [05:32<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.010             | Train Accuracy:  0.628             | Val Loss:  0.011             | Val Accuracy:  0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 46/843 [00:18<05:13,  2.54it/s]"
     ]
    }
   ],
   "source": [
    "model = SWEMClassifier(embedding_matrix=emb_matrix, pooling=\"avg\", freeze_emb=False)\n",
    "train(model=model, train_dataset=train_data, val_dataset=val_data, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1, 30417,    99,    99,    99,  7505,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0], dtype=torch.int32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = train_data[0]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22953  ,  0.35885  ,  0.58239  ,  0.25259  , -0.30344  ,\n",
       "        0.0051236, -0.28178  ,  0.41135  ,  0.47261  ,  0.82356  ,\n",
       "        0.23426  ,  1.0476   , -0.291    ,  0.047954 , -0.09221  ,\n",
       "        0.10336  , -0.14871  ,  0.055016 ,  0.53798  ,  0.066848 ,\n",
       "       -0.40651  , -0.26202  ,  0.063933 ,  0.05969  ,  0.003493 ,\n",
       "        0.79334  , -0.95705  , -0.40116  ,  0.1664   , -0.38669  ,\n",
       "       -0.55154  ,  1.2724   , -0.36677  ,  0.020613 , -0.33584  ,\n",
       "       -0.018895 , -0.094397 , -1.183    , -0.25509  , -0.071633 ,\n",
       "       -0.597    ,  0.41676  ,  0.44958  ,  0.17139  , -0.25743  ,\n",
       "        0.18268  , -0.42038  ,  0.49184  ,  0.26618  ,  0.20252  ,\n",
       "        0.03696  ,  0.36088  ,  0.34116  ,  0.226    , -0.01369  ,\n",
       "       -0.35573  , -0.023686 , -0.21707  , -0.18331  , -0.74595  ,\n",
       "        0.4819   , -0.45127  ,  0.40188  , -0.16615  , -0.0043198,\n",
       "       -0.15258  , -0.026358 ,  0.45227  , -0.44518  ,  0.303    ,\n",
       "        0.005681 , -0.55374  , -0.73395  , -0.17064  , -0.075054 ,\n",
       "       -0.2147   , -0.28613  , -0.54711  , -0.45993  , -0.069371 ,\n",
       "       -0.43478  ,  0.14729  ,  0.10942  ,  1.029    ,  0.057081 ,\n",
       "        0.077198 ,  0.16498  ,  0.47301  ,  0.14673  , -0.056605 ,\n",
       "       -0.31747  ,  0.20636  ,  0.29277  ,  0.10967  , -0.29272  ,\n",
       "       -0.25046  , -0.18608  ,  0.17242  , -0.0036911,  0.355    ])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_matrix[30417]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2295,  0.3589,  0.5824,  0.2526, -0.3034,  0.0051, -0.2818,  0.4114,\n",
       "         0.4726,  0.8236,  0.2343,  1.0476, -0.2910,  0.0480, -0.0922,  0.1034,\n",
       "        -0.1487,  0.0550,  0.5380,  0.0668, -0.4065, -0.2620,  0.0639,  0.0597,\n",
       "         0.0035,  0.7933, -0.9571, -0.4012,  0.1664, -0.3867, -0.5515,  1.2724,\n",
       "        -0.3668,  0.0206, -0.3358, -0.0189, -0.0944, -1.1830, -0.2551, -0.0716,\n",
       "        -0.5970,  0.4168,  0.4496,  0.1714, -0.2574,  0.1827, -0.4204,  0.4918,\n",
       "         0.2662,  0.2025,  0.0370,  0.3609,  0.3412,  0.2260, -0.0137, -0.3557,\n",
       "        -0.0237, -0.2171, -0.1833, -0.7459,  0.4819, -0.4513,  0.4019, -0.1662,\n",
       "        -0.0043, -0.1526, -0.0264,  0.4523, -0.4452,  0.3030,  0.0057, -0.5537,\n",
       "        -0.7340, -0.1706, -0.0751, -0.2147, -0.2861, -0.5471, -0.4599, -0.0694,\n",
       "        -0.4348,  0.1473,  0.1094,  1.0290,  0.0571,  0.0772,  0.1650,  0.4730,\n",
       "         0.1467, -0.0566, -0.3175,  0.2064,  0.2928,  0.1097, -0.2927, -0.2505,\n",
       "        -0.1861,  0.1724, -0.0037,  0.3550])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.embedding(X)\n",
    "out[1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c3b128559c7e8fd624042ca8b6c93b33cd59aca7b58d05c9d4cd21ec1a84d35"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('kogito')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
