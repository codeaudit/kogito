{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import wandb\n",
    "import spacy\n",
    "\n",
    "def create_emb_matrix(embedding_dim=100):\n",
    "    glove = pd.read_csv(f'data/glove/glove.6B.{embedding_dim}d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    vocab = {'<pad>': 0, '<unk>': 1}\n",
    "    embeddings = np.zeros((len(glove) + 2, embedding_dim))\n",
    "    embeddings[0] = np.zeros(embedding_dim)\n",
    "    embeddings[1] = np.zeros(embedding_dim)\n",
    "\n",
    "    for index, (key, val) in tqdm(enumerate(glove.T.items())):\n",
    "        vocab[key] = index + 2\n",
    "        embeddings[index+2] = val.to_numpy()\n",
    "\n",
    "    return vocab, embeddings\n",
    "\n",
    "class HeadDataset(Dataset):\n",
    "    def __init__(self, df, vocab, embedding_matrix, pooling=\"avg\"):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.labels = []\n",
    "        self.embeddings = []\n",
    "\n",
    "        for index, text in enumerate(df['text']):\n",
    "            doc = nlp(text)\n",
    "            vectors = []\n",
    "            for token in doc:\n",
    "                if token.text in vocab:\n",
    "                    vectors.append(embedding_matrix[vocab[token.text]])\n",
    "            \n",
    "            if vectors:\n",
    "                if pooling == \"max\":\n",
    "                    self.embeddings.append(np.amax(np.array(vectors, dtype=np.float32), axis=0))\n",
    "                else:\n",
    "                    self.embeddings.append(np.mean(vectors, axis=0, dtype=np.float32))\n",
    "                self.labels.append(df['label'][index])\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "\n",
    "class MaxPool(nn.Module):\n",
    "    def forward(self, X):\n",
    "        values, _ = torch.max(X, dim=1)\n",
    "        return values\n",
    "\n",
    "\n",
    "class AvgPool(nn.Module):\n",
    "    def forward(self, X):\n",
    "        return torch.mean(X, dim=1)\n",
    "\n",
    "\n",
    "class SWEM2Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=128, num_classes=3, embedding_dim=100):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        outputs = self.linear1(X)\n",
    "        outputs = self.activation1(outputs)\n",
    "        outputs = self.linear2(outputs)\n",
    "        outputs = self.softmax(outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def save_pretrained(self, path):\n",
    "        torch.save(self, path)\n",
    "\n",
    "\n",
    "def train(model, train_dataset, val_dataset, learning_rate=1e-3, epochs=10, batch_size=8):\n",
    "    # wandb.init(project=\"kogito-relation-matcher\", config={\"learning_rate\": learning_rate, \"epochs\": epochs, \"batch_size\": batch_size})\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        print(\"Using CUDA\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            model.zero_grad()\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            X = train_input.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            \n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "            \n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_input, val_label in val_dataloader:\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "                X = val_input.to(device)\n",
    "\n",
    "                output = model(X)\n",
    "\n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        train_loss = total_loss_train / len(train_dataset)\n",
    "        train_acc = total_acc_train / len(train_dataset)\n",
    "        val_loss = total_loss_val / len(val_dataset)\n",
    "        val_acc = total_acc_val / len(val_dataset)\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {train_loss: .3f} \\\n",
    "            | Train Accuracy: {train_acc: .3f} \\\n",
    "            | Val Loss: {val_loss: .3f} \\\n",
    "            | Val Accuracy: {val_acc: .3f}')\n",
    "        \n",
    "        # wandb.log({\"train_loss\": train_loss, \"train_accuracy\": train_acc, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
    "        # model.save_pretrained(f\"./models/checkpoint_{epoch_num}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:11, 33520.48it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab, emb_matrix = create_emb_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relation_modeling_utils import load_data\n",
    "\n",
    "train_df = load_data(\"data/atomic2020_data-feb2021/train.tsv\")\n",
    "dev_df = load_data(\"data/atomic2020_data-feb2021/dev.tsv\")\n",
    "train_data = HeadDataset(train_df, vocab, emb_matrix)\n",
    "val_data = HeadDataset(dev_df, vocab, emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.embeddings = np.array(train_data.embeddings, dtype=np.float32)\n",
    "val_data.embeddings = np.array(val_data.embeddings, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 8/415 [03:47<3:12:57, 28.45s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/kogito/examples/relation_modeling_swem2.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biccluster072.iccluster.epfl.ch/root/kogito/examples/relation_modeling_swem2.ipynb#ch0000008vscode-remote?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m SWEM2Classifier()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Biccluster072.iccluster.epfl.ch/root/kogito/examples/relation_modeling_swem2.ipynb#ch0000008vscode-remote?line=1'>2</a>\u001b[0m train(model\u001b[39m=\u001b[39;49mmodel, train_dataset\u001b[39m=\u001b[39;49mtrain_data, val_dataset\u001b[39m=\u001b[39;49mval_data, epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m)\n",
      "\u001b[1;32m/root/kogito/examples/relation_modeling_swem2.ipynb Cell 1'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, val_dataset, learning_rate, epochs, batch_size)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Biccluster072.iccluster.epfl.ch/root/kogito/examples/relation_modeling_swem2.ipynb#ch0000005vscode-remote?line=105'>106</a>\u001b[0m total_acc_train \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Biccluster072.iccluster.epfl.ch/root/kogito/examples/relation_modeling_swem2.ipynb#ch0000005vscode-remote?line=106'>107</a>\u001b[0m total_loss_train \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Biccluster072.iccluster.epfl.ch/root/kogito/examples/relation_modeling_swem2.ipynb#ch0000005vscode-remote?line=108'>109</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_input, train_label \u001b[39min\u001b[39;00m tqdm(train_dataloader):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Biccluster072.iccluster.epfl.ch/root/kogito/examples/relation_modeling_swem2.ipynb#ch0000005vscode-remote?line=109'>110</a>\u001b[0m     model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Biccluster072.iccluster.epfl.ch/root/kogito/examples/relation_modeling_swem2.ipynb#ch0000005vscode-remote?line=111'>112</a>\u001b[0m     train_label \u001b[39m=\u001b[39m train_label\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/kogito/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/tqdm/std.py?line=1191'>1192</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/tqdm/std.py?line=1193'>1194</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/tqdm/std.py?line=1194'>1195</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/tqdm/std.py?line=1195'>1196</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/tqdm/std.py?line=1196'>1197</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/tqdm/std.py?line=1197'>1198</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=519'>520</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=520'>521</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=523'>524</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=524'>525</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=558'>559</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=559'>560</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=560'>561</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=561'>562</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=562'>563</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:84\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=81'>82</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=82'>83</a>\u001b[0m     transposed \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[0;32m---> <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=83'>84</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=85'>86</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m~/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:84\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=81'>82</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=82'>83</a>\u001b[0m     transposed \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[0;32m---> <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=83'>84</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=85'>86</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m~/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:64\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=60'>61</a>\u001b[0m     \u001b[39mif\u001b[39;00m np_str_obj_array_pattern\u001b[39m.\u001b[39msearch(elem\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mstr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=61'>62</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem\u001b[39m.\u001b[39mdtype))\n\u001b[0;32m---> <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=63'>64</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m default_collate([torch\u001b[39m.\u001b[39mas_tensor(b) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m batch])\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=64'>65</a>\u001b[0m \u001b[39melif\u001b[39;00m elem\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m ():  \u001b[39m# scalars\u001b[39;00m\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=65'>66</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mas_tensor(batch)\n",
      "File \u001b[0;32m~/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:64\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=60'>61</a>\u001b[0m     \u001b[39mif\u001b[39;00m np_str_obj_array_pattern\u001b[39m.\u001b[39msearch(elem\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mstr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=61'>62</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem\u001b[39m.\u001b[39mdtype))\n\u001b[0;32m---> <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=63'>64</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m default_collate([torch\u001b[39m.\u001b[39;49mas_tensor(b) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m batch])\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=64'>65</a>\u001b[0m \u001b[39melif\u001b[39;00m elem\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m ():  \u001b[39m# scalars\u001b[39;00m\n\u001b[1;32m     <a href='file:///root/.conda/envs/kogito/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py?line=65'>66</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mas_tensor(batch)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SWEM2Classifier()\n",
    "train(model=model, train_dataset=train_data, val_dataset=val_data, epochs=20, batch_size=128, learning_rate=1e-4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e80d669f60bd34413df3f847d16a09f3fef6827513c4f925b04d67ca5f47b92"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('kogito')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
