{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/comet-bart-ai2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/comet-bart-ai2/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/comet-bart-ai2/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/comet-bart-ai2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/comet-bart-ai2/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/comet-bart-ai2/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/comet-bart-ai2/resolve/main/merges.txt HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/comet-bart-ai2/resolve/main/tokenizer.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/comet-bart-ai2/resolve/main/added_tokens.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/comet-bart-ai2/resolve/main/special_tokens_map.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/comet-bart-ai2/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/comet-bart-ai2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/comet-bart-ai2/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting heads...\n",
      "Matching relations...\n",
      "Generating commonsense graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.84s/it]\n"
     ]
    }
   ],
   "source": [
    "from kogito.models.bart.comet import COMETBART\n",
    "from kogito.inference import CommonsenseInference\n",
    "from kogito.core.model import KnowledgeModel\n",
    "from kogito.core.knowledge import KnowledgeGraph\n",
    "\n",
    "model: KnowledgeModel = COMETBART.from_pretrained(\"mismayil/comet-bart-ai2\")\n",
    "csi = CommonsenseInference()\n",
    "text = \"PersonX becomes a great basketball player\"\n",
    "kgraph: KnowledgeGraph = csi.infer(text, model)\n",
    "kgraph.to_jsonl(\"results/kgraph.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./comet-bart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'head': ['sentence_extractor',\n",
       "  'noun_phrase_extractor',\n",
       "  'verb_phrase_extractor'],\n",
       " 'relation': ['simple_relation_matcher', 'graph_relation_matcher']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csi.processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csi.remove_processor(\"noun_phrase_extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'head': ['sentence_extractor', 'verb_phrase_extractor'],\n",
       " 'relation': ['simple_relation_matcher', 'graph_relation_matcher']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csi.processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "\n",
    "from kogito.core.processors.head import KnowledgeHeadExtractor, KnowledgeHead\n",
    "\n",
    "class AdjectiveHeadExtractor(KnowledgeHeadExtractor):\n",
    "   def extract(self, text: str, doc: Optional[Doc] = None) -> List[KnowledgeHead]:\n",
    "      if not doc:\n",
    "            doc = self.lang(text)\n",
    "\n",
    "      heads = []\n",
    "\n",
    "      for token in doc:\n",
    "            if token.pos_ == \"ADJ\":\n",
    "               heads.append(KnowledgeHead(text=token.text, entity=token))\n",
    "\n",
    "      return heads\n",
    "\n",
    "adj_extractor = AdjectiveHeadExtractor(\"adj_extractor\", spacy.load(\"en_core_web_sm\"))\n",
    "csi.add_processor(adj_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'head': ['sentence_extractor', 'verb_phrase_extractor', 'adj_extractor'],\n",
       " 'relation': ['simple_relation_matcher', 'graph_relation_matcher']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csi.processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kogito.inference import CommonsenseInference\n",
    "from kogito.core.knowledge import KnowledgeGraph\n",
    "\n",
    "csi = CommonsenseInference()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dry-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting heads...\n",
      "Matching relations...\n"
     ]
    }
   ],
   "source": [
    "text = \"Gabby always brought cookies to school.\"\n",
    "kgraph: KnowledgeGraph = csi.infer(text, dry_run=True)\n",
    "kgraph.to_jsonl(\"results/kgraph_dry_run.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting heads...\n",
      "Matching relations...\n"
     ]
    }
   ],
   "source": [
    "text = \"I wanted to feed him. he didnt listen to me\"\n",
    "kgraph: KnowledgeGraph = csi.infer(text, dry_run=True)\n",
    "kgraph.to_jsonl(\"results/kgraph_dry_run_2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting heads...\n",
      "Matching relations...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from kogito.core.relation import OBJECT_USE, CAUSES\n",
    "text = \"Gabby always brought cookies to school.\"\n",
    "kgraph: KnowledgeGraph = csi.infer(text, dry_run=True, relations=[OBJECT_USE, CAUSES])\n",
    "kgraph.to_jsonl(\"results/kgraph_rel_subset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Head extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching relations...\n"
     ]
    }
   ],
   "source": [
    "text = \"Gabby always brought cookies to school.\"\n",
    "kgraph: KnowledgeGraph = csi.infer(text, dry_run=True, extract_heads=False)\n",
    "kgraph.to_jsonl(\"results/kgraph_no_head_extract.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Relation matching and no subset of relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting heads...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No relation found to match",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/mismayil/Desktop/EPFL/nlplab/kogito/examples/demo.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mismayil/Desktop/EPFL/nlplab/kogito/examples/demo.ipynb#ch0000019?line=0'>1</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGabby always brought cookies to school.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mismayil/Desktop/EPFL/nlplab/kogito/examples/demo.ipynb#ch0000019?line=1'>2</a>\u001b[0m kgraph: KnowledgeGraph \u001b[39m=\u001b[39m csi\u001b[39m.\u001b[39;49minfer(text, dry_run\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, match_relations\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/temp/lib/python3.8/site-packages/kogito/inference.py:155\u001b[0m, in \u001b[0;36mCommonsenseInference.infer\u001b[0;34m(self, text, model, heads, model_args, extract_heads, match_relations, relations, dry_run, sample_graph)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/mismayil/opt/anaconda3/envs/temp/lib/python3.8/site-packages/kogito/inference.py?line=150'>151</a>\u001b[0m     head_relations \u001b[39m=\u001b[39m head_relations\u001b[39m.\u001b[39munion(\n\u001b[1;32m    <a href='file:///Users/mismayil/opt/anaconda3/envs/temp/lib/python3.8/site-packages/kogito/inference.py?line=151'>152</a>\u001b[0m         \u001b[39mset\u001b[39m(\u001b[39mlist\u001b[39m(product(kg_heads, relations)))\n\u001b[1;32m    <a href='file:///Users/mismayil/opt/anaconda3/envs/temp/lib/python3.8/site-packages/kogito/inference.py?line=152'>153</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/mismayil/opt/anaconda3/envs/temp/lib/python3.8/site-packages/kogito/inference.py?line=153'>154</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/mismayil/opt/anaconda3/envs/temp/lib/python3.8/site-packages/kogito/inference.py?line=154'>155</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo relation found to match\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/mismayil/opt/anaconda3/envs/temp/lib/python3.8/site-packages/kogito/inference.py?line=156'>157</a>\u001b[0m kg_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='file:///Users/mismayil/opt/anaconda3/envs/temp/lib/python3.8/site-packages/kogito/inference.py?line=158'>159</a>\u001b[0m \u001b[39mfor\u001b[39;00m head_relation \u001b[39min\u001b[39;00m head_relations:\n",
      "\u001b[0;31mValueError\u001b[0m: No relation found to match"
     ]
    }
   ],
   "source": [
    "text = \"Gabby always brought cookies to school.\"\n",
    "kgraph: KnowledgeGraph = csi.infer(text, dry_run=True, match_relations=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Relation matching with subset of relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting heads...\n"
     ]
    }
   ],
   "source": [
    "from kogito.core.relation import DESIRES, CAUSES\n",
    "text = \"Gabby always brought cookies to school.\"\n",
    "kgraph: KnowledgeGraph = csi.infer(text, dry_run=True, match_relations=False, relations=[CAUSES, DESIRES])\n",
    "kgraph.to_jsonl(\"results/kgraph_no_match_subset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Head extraction, no Relation matching with subset of relations (hence, ultimate manual specification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kogito.core.relation import DESIRES, CAUSES\n",
    "text = \"Gabby always brought cookies to school.\"\n",
    "kgraph: KnowledgeGraph = csi.infer(text, dry_run=True, extract_heads=False, match_relations=False, relations=[CAUSES, DESIRES])\n",
    "kgraph.to_jsonl(\"results/kgraph_manual.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching relations...\n"
     ]
    }
   ],
   "source": [
    "from kogito.inference import CommonsenseInference\n",
    "from kogito.core.knowledge import KnowledgeGraph\n",
    "\n",
    "csi = CommonsenseInference()\n",
    "text = \"Gabby always brought cookies to school.\"\n",
    "kgraph: KnowledgeGraph = csi.infer(heads=[\"post office\", \"to get out of the room\"], dry_run=True)\n",
    "kgraph.to_jsonl(\"results/kgraph_manual_heads.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model based Relation matching (SWEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mismayil/opt/anaconda3/envs/temp/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/kogito-rc-swem/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/kogito-rc-swem/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /var/folders/rs/c9bqjyq95q59ngc5v1t0gz_00000gn/T/tmpwz6_d8ub\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /var/folders/rs/c9bqjyq95q59ngc5v1t0gz_00000gn/T/tmpwz6_d8ub/_remote_module_non_sriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching relations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mismayil/opt/anaconda3/envs/temp/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mismayil/opt/anaconda3/envs/temp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 59.87it/s]\n",
      "Took 0.688650369644165 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mismayil/opt/anaconda3/envs/temp/lib/python3.8/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "from kogito.inference import CommonsenseInference\n",
    "from kogito.core.knowledge import KnowledgeGraph\n",
    "from kogito.core.processors.relation import SWEMRelationMatcher\n",
    "from kogito.models.bart.comet import COMETBART\n",
    "from kogito.core.model import KnowledgeModel\n",
    "import time\n",
    "\n",
    "csi = CommonsenseInference()\n",
    "csi.remove_processor(\"simple_relation_matcher\")\n",
    "swem_matcher = SWEMRelationMatcher(\"swem_relation_matcher\")\n",
    "csi.add_processor(swem_matcher)\n",
    "start = time.time()\n",
    "# model: KnowledgeModel = COMETBART.from_pretrained(\"/Users/mismayil/Desktop/EPFL/nlplab/comet-atomic-2020/comet-atomic_2020_BART\")\n",
    "kgraph: KnowledgeGraph = csi.infer(heads=[\"banana\", \"love another\", \"Student gets a card\"], dry_run=True)\n",
    "end = time.time()\n",
    "print(f\"Took {end-start} seconds\")\n",
    "kgraph.to_jsonl(\"results/kgraph_modelbased_relations_swem.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model based Relation matching (DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/kogito-rc-distilbert/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/kogito-rc-distilbert/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /distilbert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /distilbert-base-uncased/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching relations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /distilbert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /distilbert-base-uncased/resolve/main/added_tokens.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /distilbert-base-uncased/resolve/main/special_tokens_map.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /distilbert-base-uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /distilbert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  6.56it/s]\n",
      "Took 3.07722806930542 seconds\n"
     ]
    }
   ],
   "source": [
    "from kogito.inference import CommonsenseInference\n",
    "from kogito.core.knowledge import KnowledgeGraph\n",
    "from kogito.core.processors.relation import DistilBERTRelationMatcher\n",
    "from kogito.core.model import KnowledgeModel\n",
    "import time\n",
    "\n",
    "csi = CommonsenseInference()\n",
    "csi.remove_processor(\"simple_relation_matcher\")\n",
    "dbert_matcher = DistilBERTRelationMatcher(\"dbert_relation_matcher\")\n",
    "csi.add_processor(dbert_matcher)\n",
    "start = time.time()\n",
    "# model: KnowledgeModel = COMETBART.from_pretrained(\"/Users/mismayil/Desktop/EPFL/nlplab/comet-atomic-2020/comet-atomic_2020_BART\")\n",
    "kgraph: KnowledgeGraph = csi.infer(heads=[\"banana\", \"love another\", \"Student gets a card\"], dry_run=True)\n",
    "end = time.time()\n",
    "print(f\"Took {end-start} seconds\")\n",
    "kgraph.to_jsonl(\"results/kgraph_modelbased_relations_dbert.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model based Relation Matching (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/kogito-rc-bert/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mismayil/kogito-rc-bert/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching relations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/added_tokens.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/special_tokens_map.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
      "Took 4.072222948074341 seconds\n"
     ]
    }
   ],
   "source": [
    "from kogito.inference import CommonsenseInference\n",
    "from kogito.core.knowledge import KnowledgeGraph\n",
    "from kogito.core.processors.relation import BERTRelationMatcher\n",
    "from kogito.core.model import KnowledgeModel\n",
    "import time\n",
    "\n",
    "csi = CommonsenseInference()\n",
    "csi.remove_processor(\"simple_relation_matcher\")\n",
    "bert_matcher = BERTRelationMatcher(\"dbert_relation_matcher\")\n",
    "csi.add_processor(bert_matcher)\n",
    "start = time.time()\n",
    "# model: KnowledgeModel = COMETBART.from_pretrained(\"/Users/mismayil/Desktop/EPFL/nlplab/comet-atomic-2020/comet-atomic_2020_BART\")\n",
    "kgraph: KnowledgeGraph = csi.infer(heads=[\"banana\", \"love another\", \"Student gets a card\"], dry_run=True)\n",
    "end = time.time()\n",
    "print(f\"Took {end-start} seconds\")\n",
    "kgraph.to_jsonl(\"results/kgraph_modelbased_relations_bert.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3 Based Commonsense Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mismayil/opt/anaconda3/envs/temp/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching relations...\n",
      "Generating commonsense graph...\n",
      "Took 2.3603579998016357 seconds\n"
     ]
    }
   ],
   "source": [
    "from kogito.inference import CommonsenseInference\n",
    "from kogito.core.knowledge import KnowledgeGraph\n",
    "from kogito.models.gpt3.zeroshot import GPT3Zeroshot\n",
    "import time, os\n",
    "\n",
    "csi = CommonsenseInference()\n",
    "csi.remove_processor(\"simple_relation_matcher\")\n",
    "\n",
    "model = GPT3Zeroshot(api_key=\"\", model_name=\"text-davinci-002\")\n",
    "sample_graph = KnowledgeGraph.from_csv(\"sample_graph.tsv\", sep=\"\\t\", header=None)\n",
    "heads = [\"PersonX accuses PersonY of cheating\", \"PersonX aces PersonX's exam\"]\n",
    "\n",
    "start = time.time()\n",
    "kgraph = csi.infer(model=model, heads=heads, sample_graph=sample_graph, model_args={\"debug\": True})\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Took {end-start} seconds\")\n",
    "kgraph.to_jsonl(\"results/kgraph_gpt3.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3 Based Commonsense Inference with custom relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching relations...\n",
      "Generating commonsense graph...\n",
      "Took 2.438878297805786 seconds\n"
     ]
    }
   ],
   "source": [
    "from kogito.inference import CommonsenseInference\n",
    "from kogito.core.knowledge import KnowledgeGraph\n",
    "from kogito.models.gpt3.zeroshot import GPT3Zeroshot\n",
    "from kogito.core.relation import KnowledgeRelation, register_relation\n",
    "import time, os\n",
    "\n",
    "csi = CommonsenseInference()\n",
    "csi.remove_processor(\"simple_relation_matcher\")\n",
    "\n",
    "def x_want2_verbalizer(head, **kwargs):\n",
    "    index = kwargs.get(\"index\")\n",
    "    index_txt = f\"{index}\" if index is not None else \"\"\n",
    "    return f\"Situation {index_txt}: {head}\\nWants: As a result, PersonX wants\"\n",
    "\n",
    "X_WANT2 = KnowledgeRelation(\"xWant2\",\n",
    "                            verbalizer=x_want2_verbalizer,\n",
    "                            prompt=\"How does this situation affect each character's wants?\")\n",
    "register_relation(X_WANT2)\n",
    "\n",
    "model = GPT3Zeroshot(api_key=\"\", model_name=\"text-davinci-002\")\n",
    "\n",
    "sample_graph = KnowledgeGraph.from_csv(\"sample_graph2.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "heads = [\"PersonX makes a huge mistake\", \"PersonX sees PersonY's point\"]\n",
    "\n",
    "start = time.time()\n",
    "kgraph = csi.infer(model=model,\n",
    "                   heads=heads,\n",
    "                   sample_graph=sample_graph,\n",
    "                   model_args={\"debug\": True})\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Took {end-start} seconds\")\n",
    "kgraph.to_jsonl(\"results/kgraph_gpt3_custom_relation.json\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c3b128559c7e8fd624042ca8b6c93b33cd59aca7b58d05c9d4cd21ec1a84d35"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('kogito')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
