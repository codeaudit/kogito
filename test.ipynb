{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaV2ForSequenceClassification, DebertaV2Tokenizer\n",
    "import math\n",
    "\n",
    "NARRATIVE_SEP_TOKEN = \"<n_sep>\"\n",
    "FACT_SEP_TOKEN = \"<f_sep>\"\n",
    "\n",
    "def truncate_sequences_dual(sequences, max_length):\n",
    "    words_to_cut = sum(list(map(len, sequences))) - max_length\n",
    "    if words_to_cut <= 0:\n",
    "        return sequences\n",
    "\n",
    "    words_to_cut_before = math.ceil(words_to_cut / 2.0)\n",
    "    words_to_cut_after = words_to_cut // 2\n",
    "\n",
    "    while words_to_cut_before > len(sequences[0]):\n",
    "        words_to_cut_before -= len(sequences[0])\n",
    "        sequences = sequences[1:]\n",
    "    sequences[0] = sequences[0][words_to_cut_before:]\n",
    "\n",
    "    while words_to_cut_after > len(sequences[-1]):\n",
    "        words_to_cut_after -= len(sequences[-1])\n",
    "        sequences = sequences[:-1]\n",
    "    last = len(sequences[-1]) - words_to_cut_after\n",
    "    sequences[-1] = sequences[-1][:last]\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def pad_ids(arrays, padding, max_length=-1):\n",
    "    if max_length < 0:\n",
    "        max_length = max(list(map(len, arrays)))\n",
    "    \n",
    "    arrays = [\n",
    "        array + [padding] * (max_length - len(array))\n",
    "        for array in arrays\n",
    "    ]\n",
    "\n",
    "    return arrays\n",
    "\n",
    "RELATION_VERBALIZER = {\"AtLocation\": \"located or found at/in/on\",\n",
    "                       \"CapableOf\": \"is/are capable of\",\n",
    "                       \"Causes\": \"causes\",\n",
    "                       \"CausesDesire\": \"makes someone want\",\n",
    "                       \"CreatedBy\": \"is created by\",\n",
    "                       \"Desires\": \"desires\",\n",
    "                       \"HasA\": \"has, possesses or contains\",\n",
    "                       \"HasFirstSubevent\": \"begins with the event/action\",\n",
    "                       \"HasLastSubevent\": \"ends with the event/action\",\n",
    "                       \"HasPrerequisite\": \"to do this, one requires\",\n",
    "                       \"HasProperty\": \"can be characterized by being/having\",\n",
    "                       \"HasSubEvent\": \"includes the event/action\",\n",
    "                       \"HinderedBy\": \"can be hindered by\",\n",
    "                       \"InstanceOf\": \"is an example/instance of\",\n",
    "                       \"isAfter\": \"happens after\",\n",
    "                       \"isBefore\": \"happens before\",\n",
    "                       \"isFilledBy\": \"___ can be filled by\",\n",
    "                       \"MadeOf\": \"is made of\",\n",
    "                       \"MadeUpOf\": \"made (up) of\",\n",
    "                       \"MotivatedByGoal\": \"is a step towards accomplishing the goal\",\n",
    "                       \"NotDesires\": \"do(es) not desire\",\n",
    "                       \"ObjectUse\": \"used for\",\n",
    "                       \"UsedFor\": \"used for\",\n",
    "                       \"oEffect\": \"as a result, PersonY or others will\",\n",
    "                       \"oReact\": \"as a result, PersonY or others feels\",\n",
    "                       \"oWant\": \"as a result, PersonY or others wants\",\n",
    "                       \"PartOf\": \"is a part of\",\n",
    "                       \"ReceivesAction\": \"can receive or be affected by the action\",\n",
    "                       \"xAttr\": \"PersonX is seen as\",\n",
    "                       \"xEffect\": \"as a result, PersonX will\",\n",
    "                       \"xIntent\": \"because PersonX wants\",\n",
    "                       \"xNeed\": \"but before, PersonX needs\",\n",
    "                       \"xReact\": \"as a result, PersonX feels\",\n",
    "                       \"xReason\": \"because\",\n",
    "                       \"xWant\": \"as a result, PersonX wants\"}\n",
    "\n",
    "tokenizer_path = model_path = \"ComFact_DeBERTa/deberta-large-nlu-fact_full/checkpoint-236560\"\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(tokenizer_path)\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "narrative_sep_id = tokenizer.convert_tokens_to_ids(NARRATIVE_SEP_TOKEN)\n",
    "fact_sep_id = tokenizer.convert_tokens_to_ids(FACT_SEP_TOKEN)\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "context = [\n",
    "      \"hey , i am in a lady motorcycle club and i love to drive fast\",\n",
    "      \"i am married to a wife beater and have two kids\",\n",
    "      \"well do you want me to come beat him ? i have never lost a fight\",\n",
    "      \"then we can go shopping ! i love shopping . i am a lifestyle shop blogger .\",\n",
    "      \"well there you go lol and your kids would enjoy checking my tatts i have got 12\",\n",
    "      \"i am very attractive . i was a cheerleader in high school . maybe we can go on a date\",\n",
    "      \"u like women too ? did not know that\",\n",
    "      \"got to get away from my husband i live in florida . celebration florida come meet me\",\n",
    "      \"i just drove 20 mins this morning at 208 mph i can get there fast\",\n",
    "      \"i will leave my kids never liked them lets do this !\",\n",
    "      \"sounds like a plan i will be there soon you can hop on my bike\",\n",
    "      \"the we can ride off into the sunset just like lovers in a novel\",\n",
    "      \"well then pack your bags\",\n",
    "      \"yay i am so excited i think i will burn the house down before i leave .\"\n",
    "    ]\n",
    "# fact = {\"head\": \"PersonX drives ___ fast\", \"relation\": \"xIntent\", \"tail\": \"to get a thrill\"}\n",
    "fact = {\"head\": \"PersonX drives ___ fast\", \"relation\": \"oWant\", \"tail\": \"to call the police\"}\n",
    "fact[\"relation\"] = RELATION_VERBALIZER[fact[\"relation\"]]\n",
    "fact[\"head\"] = fact[\"head\"].lower()\n",
    "fact[\"tail\"] = fact[\"tail\"].lower()\n",
    "\n",
    "context_ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) for sent in context]\n",
    "fact_ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(fact[key])) for key in [\"head\", \"relation\", \"tail\"]]\n",
    "\n",
    "no_trunc_token_num = 1 + len(context_ids) + len(fact_ids)  # [CLS], [SEP], <d_sep> and <s_sep>\n",
    "for node in fact_ids:\n",
    "    no_trunc_token_num += len(node)  # do not truncate statement\n",
    "\n",
    "max_tokens = 512\n",
    "truncated_context_ids = truncate_sequences_dual(context_ids, max_tokens-no_trunc_token_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128001, 128002)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_sep_id, fact_sep_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[604, 982, 5328, 5179, 616, 616, 1274],\n",
       " [283, 266, 868, 261, 12590, 3021, 289, 690, 1654],\n",
       " [264, 660, 262, 1164]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(ids) for ids in context_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(ids) for ids in truncated_context_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_ids = truncated_context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "context_ids_with_sep = list(chain(*[ids+[narrative_sep_id] for ids in context_ids[:-1]], context_ids[-1]))\n",
    "fact_ids_with_sep = list(chain(*[ids+[fact_sep_id] for ids in fact_ids[:-1]], fact_ids[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.build_inputs_with_special_tokens(context_ids_with_sep, fact_ids_with_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 11187,\n",
       " 366,\n",
       " 584,\n",
       " 481,\n",
       " 267,\n",
       " 266,\n",
       " 4396,\n",
       " 8209,\n",
       " 1788,\n",
       " 263,\n",
       " 584,\n",
       " 472,\n",
       " 264,\n",
       " 1168,\n",
       " 1274,\n",
       " 128001,\n",
       " 584,\n",
       " 481,\n",
       " 2410,\n",
       " 264,\n",
       " 266,\n",
       " 1553,\n",
       " 56596,\n",
       " 263,\n",
       " 286,\n",
       " 375,\n",
       " 978,\n",
       " 128001,\n",
       " 371,\n",
       " 333,\n",
       " 274,\n",
       " 409,\n",
       " 351,\n",
       " 264,\n",
       " 488,\n",
       " 2584,\n",
       " 417,\n",
       " 1102,\n",
       " 584,\n",
       " 286,\n",
       " 518,\n",
       " 1125,\n",
       " 266,\n",
       " 1801,\n",
       " 128001,\n",
       " 393,\n",
       " 301,\n",
       " 295,\n",
       " 424,\n",
       " 2017,\n",
       " 1084,\n",
       " 584,\n",
       " 472,\n",
       " 2017,\n",
       " 323,\n",
       " 584,\n",
       " 481,\n",
       " 266,\n",
       " 3444,\n",
       " 1638,\n",
       " 8874,\n",
       " 323,\n",
       " 128001,\n",
       " 371,\n",
       " 343,\n",
       " 274,\n",
       " 424,\n",
       " 8878,\n",
       " 263,\n",
       " 290,\n",
       " 978,\n",
       " 338,\n",
       " 929,\n",
       " 4155,\n",
       " 312,\n",
       " 33276,\n",
       " 297,\n",
       " 268,\n",
       " 584,\n",
       " 286,\n",
       " 519,\n",
       " 621,\n",
       " 128001,\n",
       " 584,\n",
       " 481,\n",
       " 379,\n",
       " 3851,\n",
       " 323,\n",
       " 584,\n",
       " 284,\n",
       " 266,\n",
       " 43701,\n",
       " 267,\n",
       " 459,\n",
       " 563,\n",
       " 323,\n",
       " 1461,\n",
       " 301,\n",
       " 295,\n",
       " 424,\n",
       " 277,\n",
       " 266,\n",
       " 1043,\n",
       " 128001,\n",
       " 3636,\n",
       " 334,\n",
       " 694,\n",
       " 461,\n",
       " 1102,\n",
       " 464,\n",
       " 298,\n",
       " 391,\n",
       " 272,\n",
       " 128001,\n",
       " 519,\n",
       " 264,\n",
       " 350,\n",
       " 557,\n",
       " 292,\n",
       " 312,\n",
       " 1745,\n",
       " 584,\n",
       " 685,\n",
       " 267,\n",
       " 29983,\n",
       " 323,\n",
       " 4630,\n",
       " 29983,\n",
       " 488,\n",
       " 957,\n",
       " 351,\n",
       " 128001,\n",
       " 584,\n",
       " 348,\n",
       " 5066,\n",
       " 602,\n",
       " 12178,\n",
       " 291,\n",
       " 1066,\n",
       " 288,\n",
       " 23299,\n",
       " 9353,\n",
       " 584,\n",
       " 295,\n",
       " 350,\n",
       " 343,\n",
       " 1274,\n",
       " 128001,\n",
       " 584,\n",
       " 296,\n",
       " 1021,\n",
       " 312,\n",
       " 978,\n",
       " 518,\n",
       " 3172,\n",
       " 349,\n",
       " 4573,\n",
       " 333,\n",
       " 291,\n",
       " 1084,\n",
       " 128001,\n",
       " 2163,\n",
       " 334,\n",
       " 266,\n",
       " 741,\n",
       " 584,\n",
       " 296,\n",
       " 282,\n",
       " 343,\n",
       " 950,\n",
       " 274,\n",
       " 295,\n",
       " 8761,\n",
       " 277,\n",
       " 312,\n",
       " 2772,\n",
       " 128001,\n",
       " 262,\n",
       " 301,\n",
       " 295,\n",
       " 2224,\n",
       " 442,\n",
       " 352,\n",
       " 262,\n",
       " 9606,\n",
       " 348,\n",
       " 334,\n",
       " 7462,\n",
       " 267,\n",
       " 266,\n",
       " 2626,\n",
       " 128001,\n",
       " 371,\n",
       " 393,\n",
       " 3105,\n",
       " 290,\n",
       " 3713,\n",
       " 128001,\n",
       " 39390,\n",
       " 584,\n",
       " 481,\n",
       " 324,\n",
       " 2199,\n",
       " 584,\n",
       " 428,\n",
       " 584,\n",
       " 296,\n",
       " 5134,\n",
       " 262,\n",
       " 669,\n",
       " 444,\n",
       " 416,\n",
       " 584,\n",
       " 1021,\n",
       " 323,\n",
       " 2,\n",
       " 604,\n",
       " 982,\n",
       " 5328,\n",
       " 5179,\n",
       " 616,\n",
       " 616,\n",
       " 1274,\n",
       " 128002,\n",
       " 283,\n",
       " 266,\n",
       " 868,\n",
       " 261,\n",
       " 12590,\n",
       " 3021,\n",
       " 289,\n",
       " 690,\n",
       " 1654,\n",
       " 128002,\n",
       " 264,\n",
       " 660,\n",
       " 262,\n",
       " 1164,\n",
       " 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/kogito/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_scores = torch.bmm(query_layer, key_layer.transpose(-1, -2)) / torch.tensor(\n",
      "/root/.conda/envs/kogito/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:829: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += c2p_att / torch.tensor(scale, dtype=c2p_att.dtype)\n",
      "/root/.conda/envs/kogito/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:852: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "input_ids = torch.tensor(pad_ids(input_ids, pad_token_id))\n",
    "\n",
    "output = model(torch.tensor(input_ids).unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6571, -3.7280]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9938e-01, 6.2003e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(output.logits, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kogito')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e80d669f60bd34413df3f847d16a09f3fef6827513c4f925b04d67ca5f47b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
